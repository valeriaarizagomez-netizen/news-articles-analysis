# -*- coding: utf-8 -*-
"""03_stance_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DlIr9pafcm_HUHG-83xJtDLE7jYAthPu
"""

"""
03_stance_analysis.py

Purpose:
    Run zero-shot stance detection on article text for a given target group
    (e.g. "religious people", "migrants", "LGBTQ+ people", "disabled people"),
    using HuggingFace's zero-shot classification model.

Usage:
    - Place this file in code/03_stance_analysis.py
    - Make sure your project root contains:
        data/processed/<GROUP>/*.RDS      (RDS files with article text)
        output/stance/                    (will be created if not present)
    - Edit TARGET_NAME and INPUT_FOLDER below.
    - From the project root, run:
        python code/03_stance_analysis.py

Expected input:
    - One or more .RDS files in INPUT_FOLDER.
    - Each RDS must contain a text column (ideally 'body') and, optionally, a
      year or date column. The script tries to infer 'year' automatically.

Output:
    - output/stance/stance_predictions_<slugified_target>.csv
      with columns:
        text, year (if available), target, stance_label, stance_score
"""

import os
import re
import time

import pandas as pd
import pyreadr
import torch
from tqdm.auto import tqdm
from datasets import Dataset
from transformers import pipeline
from transformers.pipelines.pt_utils import KeyDataset


# Choose which group for analysis (change and re-run per group):
TARGET_NAME = "religious people"

# Folder containing the RDS files for this group, relative to project root

INPUT_FOLDER = "data/processed/Religious"

# Output directory for stance predictions (relative to project root)
OUTPUT_DIR = "output/stance"


# -----------------------------
# ensure a 'year' column
def ensure_year_column(df: pd.DataFrame) -> pd.DataFrame:
    """
    Try to derive a 'year' column from typical year or date columns.
    Returns the modified DataFrame (or unchanged if no usable column found).
    """
    # 1) Direct year-like columns
    year_candidates = [
        "year", "Year", "YEAR", "publication_year", "pub_year",
        "pubYear", "year_published"
    ]
    for c in year_candidates:
        if c in df.columns:
            y = pd.to_numeric(df[c], errors="coerce")
            if y.notna().sum() > 0:
                df["year"] = y
                return df

    # 2) Date/time columns we can parse to extract year
    date_candidates = [
        "date", "Date", "created_at", "published_at", "timestamp",
        "pub_date", "datetime", "time"
    ]
    for c in date_candidates:
        if c in df.columns:
            y = pd.to_datetime(df[c], errors="coerce", utc=True).dt.year
            if y.notna().sum() > 0:
                df["year"] = y
                return df

    # Nothing usable found; return unchanged
    return df


# -----------------------------
# auto-detect text column
def autodetect_text_col(df: pd.DataFrame) -> str:
    """
    Guess which column contains text. Prefer common names, otherwise fall back
    to the first object (string) column.
    """
    candidates = [
        "text", "tweet", "content", "body", "message",
        "headline", "title", "article"
    ]
    for c in candidates:
        if c in df.columns:
            return c
    for col in df.columns:
        if df[col].dtype == object:
            return col
    return None


# -----------------------------
# file-name safe slug
def slugify(s: str) -> str:
    s = s.lower().replace("+", "plus")
    s = re.sub(r"[^a-z0-9]+", "_", s).strip("_")
    return s


def main():
    # -------------------------
    # 1) Load and combine RDS files

    if not os.path.isdir(INPUT_FOLDER):
        raise FileNotFoundError(f"Input folder not found: {INPUT_FOLDER}")

    all_files = os.listdir(INPUT_FOLDER)
    rds_files = [f for f in all_files if f.lower().endswith(".rds")]

    if not rds_files:
        raise FileNotFoundError(f"No .RDS files found in {INPUT_FOLDER}")

    combined_df = pd.DataFrame()

    print(f"Loading RDS files from: {INPUT_FOLDER}")
    for rds_file in tqdm(rds_files, desc="Loading RDS files"):
        file_path = os.path.join(INPUT_FOLDER, rds_file)
        try:
            result = pyreadr.read_r(file_path)
            df = next(iter(result.values()))
            df = ensure_year_column(df)
            combined_df = pd.concat([combined_df, df], ignore_index=True)
            print(f"Loaded {rds_file} (rows: {len(df)})")
        except Exception as e:
            print(f"Error reading {rds_file}: {e}")

    print("All RDS files combined.")
    print(f"Total rows in combined_df: {len(combined_df)}")

    # After combining, try once more to derive year if possible
    combined_df = ensure_year_column(combined_df)

    # Make 'year' a nullable integer for clean CSVs
    if "year" in combined_df.columns:
        combined_df["year"] = pd.to_numeric(combined_df["year"], errors="coerce").astype("Int64")

    # -------------------------
    # 2) Device info
    device_str = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device_str}")

    # -------------------------
    # 3) Detect and clean text column
    text_col = autodetect_text_col(combined_df)
    if "body" in combined_df.columns:
        text_col = "body"  # prefer 'body' if present

    if not text_col:
        raise ValueError("Could not detect a text column in the data.")

    print(f"Using text column: {text_col}")

    combined_df[text_col] = (
        combined_df[text_col].astype(str)
        .str.replace(r"\s+", " ", regex=True)
        .str.strip()
    )

    # Keep rows with non-empty text
    combined_df = combined_df[combined_df[text_col].str.len() > 0].reset_index(drop=True)
    print(f"Total rows after removing empty text: {len(combined_df)}")

    # -------------------------
    # 4) Build a Dataset for speed
    cols_to_keep = [text_col]
    if "year" in combined_df.columns:
        cols_to_keep.append("year")

    work_df = combined_df[cols_to_keep].rename(columns={text_col: "text"}).copy()
    work_df["text"] = work_df["text"].astype(str)

    ds = Dataset.from_pandas(work_df, preserve_index=False)

    print(f"Total rows to analyze: {len(ds)}")
    print("Has 'year'?:", "year" in work_df.columns)
    if "year" in work_df.columns:
        print("Year null %:", work_df["year"].isna().mean())

    # -------------------------
    # 5) Zero-shot stance pipeline
    candidate_labels = [
        f"supporting {TARGET_NAME}",
        f"opposing {TARGET_NAME}",
        f"neutral toward {TARGET_NAME}"
    ]

    print(f"Target: {TARGET_NAME}")
    print("Candidate labels:", candidate_labels)

    zshot = pipeline(
        "zero-shot-classification",
        model="facebook/bart-large-mnli",
        device=0 if torch.cuda.is_available() else -1
    )

    BATCH_SIZE = 32 if torch.cuda.is_available() else 8
    CHECKPOINT_EVERY = 20000

    target_slug = slugify(TARGET_NAME)
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    predictions_path = os.path.join(OUTPUT_DIR, f"stance_predictions_{target_slug}.csv")

    pred_labels, pred_scores = [], []
    start = time.time()

    text_stream = KeyDataset(ds, "text")

    print("Starting zero-shot stance classification...")
    for i, r in enumerate(
        tqdm(
            zshot(
                text_stream,
                batch_size=BATCH_SIZE,
                truncation=True,
                candidate_labels=candidate_labels,
                multi_label=False
            ),
            total=len(ds),
            desc=f"Zero-shot ({TARGET_NAME})"
        )
    ):
        top_label, top_score = r["labels"][0], r["scores"][0]

        # Normalize stance to one of: supporting, opposing, neutral
        if top_label.startswith("supporting"):
            norm = "supporting"
        elif top_label.startswith("opposing"):
            norm = "opposing"
        else:
            norm = "neutral"

        pred_labels.append(norm)
        pred_scores.append(top_score)

        # Periodic checkpoint (large datasets)
        if (i + 1) % CHECKPOINT_EVERY == 0:
            checkpoint_df = pd.DataFrame({
                "text": work_df["text"].iloc[: i + 1].tolist(),
                "year": work_df["year"].iloc[: i + 1] if "year" in work_df.columns else None,
                "target": TARGET_NAME,
                "stance_label": pred_labels,
                "stance_score": pred_scores
            })
            checkpoint_df.to_csv(predictions_path, index=False)
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

    elapsed = time.time() - start

    # Final save
    out_df = pd.DataFrame({
        "text": work_df["text"],
        "year": work_df["year"] if "year" in work_df.columns else None,
        "target": TARGET_NAME,
        "stance_label": pred_labels,
        "stance_score": pred_scores
    })
    out_df.to_csv(predictions_path, index=False)

    print(f"Saved {len(out_df)} predictions to {predictions_path}")
    print(f"Elapsed: {elapsed/60:.1f} min | batch_size={BATCH_SIZE} | target='{TARGET_NAME}'")


if __name__ == "__main__":
    main()